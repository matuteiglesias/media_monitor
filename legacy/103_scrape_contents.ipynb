{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pyperclip\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "# Configurations\n",
    "MASTER_INDEX_PATH = './data/master_index.csv'\n",
    "SCRAPED_LOG_PATH = './data/scraped_links.jsonl'\n",
    "SLEEP_TIME = 5  # seconds\n",
    "PAGE_TIMEOUT = 15  # seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_scraped_index_ids(scraped_log_path):\n",
    "    \"\"\"Load already scraped index_ids from JSONL.\"\"\"\n",
    "    if not os.path.exists(scraped_log_path):\n",
    "        return set()\n",
    "    scraped_ids = set()\n",
    "    with open(scraped_log_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                scraped_ids.add(record['index_id'])\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error parsing line: {e}\")\n",
    "    return scraped_ids\n",
    "\n",
    "def scrape_article(row):\n",
    "    \"\"\"Launch browser, scrape article text via Ctrl+A Ctrl+C, and return enriched record.\"\"\"\n",
    "    options = Options()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    # options.add_argument(\"--headless\")  # Uncomment to run without UI\n",
    "\n",
    "    driver = None\n",
    "    result = None\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.set_page_load_timeout(PAGE_TIMEOUT)\n",
    "\n",
    "        driver.get(row['Link'])\n",
    "        time.sleep(SLEEP_TIME)  # Let dynamic content load\n",
    "\n",
    "        body = driver.find_element(By.TAG_NAME, \"body\")\n",
    "        body.send_keys(Keys.CONTROL, 'a')\n",
    "        body.send_keys(Keys.CONTROL, 'c')\n",
    "        time.sleep(1)  # Clipboard delay\n",
    "\n",
    "        scraped_html = pyperclip.paste()\n",
    "\n",
    "        result = row.to_dict()\n",
    "        result['scraped_data'] = scraped_html\n",
    "        print(f\"‚úÖ Fetched data from: {row['Link']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error scraping URL: {row['Link']}\\n{e}\")\n",
    "\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "    return result\n",
    "\n",
    "def append_scraped_records(scraped_records, scraped_log_path):\n",
    "    \"\"\"Append new scraped records to the JSONL log.\"\"\"\n",
    "    with open(scraped_log_path, 'a', encoding='utf-8') as f:\n",
    "        for record in scraped_records:\n",
    "            json.dump(record, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "def main(day=None, hour=None):\n",
    "    # Load master index\n",
    "    if not os.path.exists(MASTER_INDEX_PATH):\n",
    "        print(f\"‚ùå Master index file not found at {MASTER_INDEX_PATH}. Exiting.\")\n",
    "        return\n",
    "\n",
    "    master_df = pd.read_csv(MASTER_INDEX_PATH)\n",
    "\n",
    "    if master_df.empty or 'Published' not in master_df.columns:\n",
    "        print(\"‚ö†Ô∏è Master index is empty or missing 'Published' column. Exiting.\")\n",
    "        return\n",
    "\n",
    "    master_df['Published'] = pd.to_datetime(master_df['Published'], errors='coerce')\n",
    "\n",
    "    # Filter by day/hour\n",
    "    if day is None:\n",
    "        # Default to today in UTC\n",
    "        now_utc = datetime.now(timezone.utc)\n",
    "        day = now_utc.strftime('%Y-%m-%d')\n",
    "        hour = now_utc.hour\n",
    "\n",
    "    filter_str = f\"{day}\"\n",
    "    if hour is not None:\n",
    "        filter_str += f\" hour {hour}\"\n",
    "\n",
    "    print(f\"üîé Filtering articles published on: {filter_str}\")\n",
    "\n",
    "    filtered_df = master_df[master_df['Published'].dt.strftime('%Y-%m-%d') == day]\n",
    "    if hour is not None:\n",
    "        filtered_df = filtered_df[filtered_df['Published'].dt.hour == hour]\n",
    "\n",
    "    print(f\"üîé Found {len(filtered_df)} articles matching time filter.\")\n",
    "\n",
    "    # Load already scraped index_ids\n",
    "    scraped_ids = load_scraped_index_ids(SCRAPED_LOG_PATH)\n",
    "    print(f\"üîé Found {len(scraped_ids)} already scraped articles.\")\n",
    "\n",
    "    # Filter unscraped articles\n",
    "    unscraped_df = filtered_df[~filtered_df['index_id'].isin(scraped_ids)]\n",
    "    print(f\"üöÄ Found {len(unscraped_df)} articles to scrape.\")\n",
    "\n",
    "    scraped_results = []\n",
    "\n",
    "    for _, row in tqdm(unscraped_df.iterrows(), total=len(unscraped_df)):\n",
    "        result = scrape_article(row)\n",
    "        if result:\n",
    "            scraped_results.append(result)\n",
    "\n",
    "    # Append new results to log\n",
    "    if scraped_results:\n",
    "        append_scraped_records(scraped_results, SCRAPED_LOG_PATH)\n",
    "        print(f\"‚úÖ Appended {len(scraped_results)} new articles to {SCRAPED_LOG_PATH}.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No new articles were scraped this run.\")    # Load master index\n",
    "    if not os.path.exists(MASTER_INDEX_PATH):\n",
    "        print(f\"‚ùå Master index file not found at {MASTER_INDEX_PATH}. Exiting.\")\n",
    "        return\n",
    "\n",
    "    master_df = pd.read_csv(MASTER_INDEX_PATH)\n",
    "\n",
    "    # Load already scraped index_ids\n",
    "    scraped_ids = load_scraped_index_ids(SCRAPED_LOG_PATH)\n",
    "    print(f\"üîé Found {len(scraped_ids)} already scraped articles.\")\n",
    "\n",
    "    # Filter unscraped articles\n",
    "    unscraped_df = master_df[~master_df['index_id'].isin(scraped_ids)]\n",
    "    print(f\"üöÄ Found {len(unscraped_df)} articles to scrape.\")\n",
    "\n",
    "    scraped_results = []\n",
    "\n",
    "    for _, row in tqdm(unscraped_df.iterrows(), total=len(unscraped_df)):\n",
    "        result = scrape_article(row)\n",
    "        if result:\n",
    "            scraped_results.append(result)\n",
    "\n",
    "    # Append new results to log\n",
    "    if scraped_results:\n",
    "        append_scraped_records(scraped_results, SCRAPED_LOG_PATH)\n",
    "        print(f\"‚úÖ Appended {len(scraped_results)} new articles to {SCRAPED_LOG_PATH}.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No new articles were scraped this run.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Detected Jupyter environment. Running with default settings.\n",
      "üîé Filtering articles published on: 2025-06-11 hour 17\n",
      "üîé Found 0 articles matching time filter.\n",
      "üîé Found 0 already scraped articles.\n",
      "üöÄ Found 0 articles to scrape.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è No new articles were scraped this run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Found 0 already scraped articles.\n",
      "üöÄ Found 6350 articles to scrape.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6350 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        get_ipython\n",
    "        IN_JUPYTER = True\n",
    "    except NameError:\n",
    "        IN_JUPYTER = False\n",
    "\n",
    "    if IN_JUPYTER:\n",
    "        # Running inside a notebook\n",
    "        print(\"‚ö†Ô∏è Detected Jupyter environment. Running with default settings.\")\n",
    "        main()\n",
    "    else:\n",
    "        # Running as a standalone script\n",
    "        import argparse\n",
    "        parser = argparse.ArgumentParser(description=\"Scrape articles from master index using Selenium.\")\n",
    "        parser.add_argument('--day', type=str, help=\"Filter articles by day (YYYY-MM-DD). Defaults to today UTC.\")\n",
    "        parser.add_argument('--hour', type=int, help=\"Filter articles by hour (0-23). Defaults to current UTC hour.\")\n",
    "        args = parser.parse_args()\n",
    "\n",
    "        if args.hour is not None and not (0 <= args.hour <= 23):\n",
    "            print(\"‚ö†Ô∏è Hour must be between 0 and 23. Exiting.\")\n",
    "        else:\n",
    "            main(day=args.day, hour=args.hour)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üîç Key Features\n",
    "\n",
    "‚úÖ Reads master index and logs.\n",
    "‚úÖ Detects unscraped articles efficiently.\n",
    "‚úÖ Keeps all relevant metadata from master index (redundant but practical).\n",
    "‚úÖ Appends results incrementally ‚Äî safe for repeated runs.\n",
    "‚úÖ Easily pluggable into a scheduler or cron.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Optional Improvements\n",
    "\n",
    "üî∏ **Rate Limiting**: Add `time.sleep(2)` between scrapes to avoid getting flagged by servers.\n",
    "üî∏ **Structured Logging**: Save logs or errors for debugging.\n",
    "üî∏ **Retry Mechanism**: Keep track of failed scrapes for future attempts.\n",
    "üî∏ **Scraping with driver.page\\_source**: More reliable than Ctrl+C on some pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 134 CSV files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# Directory path\n",
    "directory = '/home/matias/Documents/media_monitor/data/rss_slices/'\n",
    "\n",
    "# Find all CSV files in the directory\n",
    "file_pattern = os.path.join(directory, '*.csv')\n",
    "csv_files = glob.glob(file_pattern)\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to hold DataFrames\n",
    "dfs = []\n",
    "\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates on all columns\n",
    "deduped_df = combined_df.drop_duplicates()\n",
    "\n",
    "# OR drop duplicates based on specific columns\n",
    "deduped_df = combined_df.drop_duplicates(subset=['Title', 'Source'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6350, 7)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduped_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3959512/788144331.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  deduped_df['day'] = pd.to_datetime(deduped_df['Published'], format='mixed').dt.strftime('%Y-%m-%d')\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "day",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "cadb9bca-207d-400f-97f9-bac8b8d392a6",
       "rows": [
        [
         "2025-05-13",
         "1"
        ],
        [
         "2025-05-14",
         "7"
        ],
        [
         "2025-05-15",
         "3"
        ],
        [
         "2025-05-16",
         "2"
        ],
        [
         "2025-05-17",
         "1"
        ],
        [
         "2025-05-18",
         "4"
        ],
        [
         "2025-05-19",
         "1"
        ],
        [
         "2025-05-20",
         "6"
        ],
        [
         "2025-05-21",
         "5"
        ],
        [
         "2025-05-22",
         "1"
        ],
        [
         "2025-05-23",
         "9"
        ],
        [
         "2025-05-24",
         "9"
        ],
        [
         "2025-05-25",
         "18"
        ],
        [
         "2025-05-26",
         "92"
        ],
        [
         "2025-05-27",
         "94"
        ],
        [
         "2025-05-28",
         "246"
        ],
        [
         "2025-05-29",
         "452"
        ],
        [
         "2025-05-30",
         "473"
        ],
        [
         "2025-05-31",
         "258"
        ],
        [
         "2025-06-01",
         "222"
        ],
        [
         "2025-06-02",
         "466"
        ],
        [
         "2025-06-03",
         "498"
        ],
        [
         "2025-06-04",
         "546"
        ],
        [
         "2025-06-05",
         "550"
        ],
        [
         "2025-06-06",
         "499"
        ],
        [
         "2025-06-07",
         "277"
        ],
        [
         "2025-06-08",
         "204"
        ],
        [
         "2025-06-09",
         "483"
        ],
        [
         "2025-06-10",
         "483"
        ],
        [
         "2025-06-11",
         "340"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 30
       }
      },
      "text/plain": [
       "day\n",
       "2025-05-13      1\n",
       "2025-05-14      7\n",
       "2025-05-15      3\n",
       "2025-05-16      2\n",
       "2025-05-17      1\n",
       "2025-05-18      4\n",
       "2025-05-19      1\n",
       "2025-05-20      6\n",
       "2025-05-21      5\n",
       "2025-05-22      1\n",
       "2025-05-23      9\n",
       "2025-05-24      9\n",
       "2025-05-25     18\n",
       "2025-05-26     92\n",
       "2025-05-27     94\n",
       "2025-05-28    246\n",
       "2025-05-29    452\n",
       "2025-05-30    473\n",
       "2025-05-31    258\n",
       "2025-06-01    222\n",
       "2025-06-02    466\n",
       "2025-06-03    498\n",
       "2025-06-04    546\n",
       "2025-06-05    550\n",
       "2025-06-06    499\n",
       "2025-06-07    277\n",
       "2025-06-08    204\n",
       "2025-06-09    483\n",
       "2025-06-10    483\n",
       "2025-06-11    340\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deduped_df['day'] = pd.to_datetime(deduped_df['Published'], format='mixed').dt.strftime('%Y-%m-%d')\n",
    "\n",
    "deduped_df.groupby('day').size().tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = deduped_df.sort_values('day').tail(20)['Link'].values\n",
    "links = list(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñå         | 1/20 [00:09<03:06,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fetched data from: https://news.google.com/rss/articles/CBMi3gFBVV95cUxPZVhqQW1JV1BkM3MwaUs0SzJnZjFxWG1iRHliTl9HRV9qVHlEUUM0RW1KbXpQWlV1cnAzc2oxNHdEcmZGOUZTMWMyR000V2hYOHNQUlczTVJ3Ty1GaF9XZ0daaGI4ZGQ1THhwTzdmTndoNzY4SnNXNzZDUDdfZHhqdUFESFJPRjNYcFhTaFdRajJaYnhRbC1ma2QwejB0RlhIcVdRdXltWmMyYUhMMHdNQ0hBS2hxZE5SamR0MTVGTm1BUjE3bkFrbkNma1FFZnZ4cjlDbkVJSkJNX3FnUGfSAd4BQVVfeXFMT2VYakFtSVdQZDNzMGlLNEsyZ2YxcVhtYkR5Yk5fR0VfalR5RFFDNEVtSm16UFpVdXJwM3NqMTR3RHJmRjlGUzFjMkdNNFdoWDhzUFJXM01Sd08tRmhfV2dHWmhiOGRkNUx4cE83Zk53aDc2OEpzVzc2Q1A3X2R4anVBREhST0YzWHBYU2hXUWoyWmJ4UWwtZmtkMHowdEZYSHFXUXV5bVpjMmFITDB3TUNIQUtocWROUmpkdDE1Rk5tQVIxN25Ba25DZmtRRWZ2eHI5Q25FSUpCTV9xZ1Bn?oc=5\n",
      "‚úÖ Fetched data from: https://news.google.com/rss/articles/CBMiwwFBVV95cUxQS0hCQ1J1UWl5SzN2QjQyel9XczMzTTg5TmhOeUVZLXFHT0o4NGlEclYtb0k2d0RoajVfMXk4VmwtTDFnTm1hSVJWVW1kdDh2b25EdjlLWkRtcW9pYkFnbU9SMFVNN2hUX1lWZGRPcEJZN21sMDJvRmZqMTkzNTY5TTVLWWhvU1p6THl1Qks0ZWdqbUg4Y3NTaFF4ZjVxN1pUX2JXUGJDSVdDcXNFck1lRVpxYXdsQmYzWUQ5Wk1CUGc3d1nSAcgBQVVfeXFMUDRTNUlIS2VBU2hVcUJFUldsMHpGYXpjVjJmVFF1OUF2QWdsblRfS3V6TUVWb09aMnNtNUI4Y3NGUzR3SDNSUVNRemYyUE9xa0FQM0NzNkhTVlRnQ01FRFNEeHBuU29yVDlBWjF2SHlaLTEtSVlNT2puaThpeUg0VDZsekdPU1AxcGt5SElILWQ4RXZ4bFZXZllYdU1Qa1VPTjN6TzNZcTUwUUhXYzlOMktjeW1XN21pVjZWYmktWF8tN0NJeUtwRV8?oc=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà         | 2/20 [00:35<05:41, 18.98s/it]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pyperclip\n",
    "from tqdm import tqdm\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# links = [\n",
    "#     \"https://example.com/page1\",\n",
    "#     \"https://example.com/page2\",\n",
    "#     # ...\n",
    "# ]\n",
    "\n",
    "output_file = 'scraped_links.jsonl'\n",
    "sleep_time = 5  # seconds\n",
    "page_timeout = 15  # seconds\n",
    "\n",
    "scraped_results = []\n",
    "\n",
    "for idx, url in enumerate(tqdm(links)):\n",
    "    options = Options()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    # options.add_argument(\"--headless\")  # Uncomment to run without UI\n",
    "\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.set_page_load_timeout(page_timeout)\n",
    "\n",
    "        driver.get(url)\n",
    "        time.sleep(sleep_time)  # Let dynamic content load\n",
    "\n",
    "        body = driver.find_element(By.TAG_NAME, \"body\")\n",
    "        body.send_keys(Keys.CONTROL, 'a')\n",
    "        body.send_keys(Keys.CONTROL, 'c')\n",
    "        time.sleep(1)  # Clipboard delay\n",
    "\n",
    "        scraped_html = pyperclip.paste()\n",
    "\n",
    "        scraped_results.append({\n",
    "            'index': idx,\n",
    "            'url': url,\n",
    "            'scraped_data': scraped_html\n",
    "        })\n",
    "        print(f\"‚úÖ Fetched data from: {url}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error scraping URL: {url}\\n{e}\")\n",
    "\n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "\n",
    "# Save results\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for record in scraped_results:\n",
    "        json.dump(record, f, ensure_ascii=False)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"Done! {len(scraped_results)} pages saved to {output_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
